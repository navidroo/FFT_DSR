Guided Depth Super-Resolution by Deep Anisotropic Diffusion

Nando Metzger*	Rodrigo Caye Daudt∗	Konrad Schindler Photogrammetry and Remote Sensing, ETH Zurich
{metzgern,rcayedaudt,schindler}@ethz.ch

Abstract
Performing super-resolution of a depth image using the guidance from an RGB image is a problem that con- cerns several fields, such as robotics, medical imaging, and remote sensing. While deep learning methods have achieved good results in this problem, recent work high- lighted the value of combining modern methods with more formal frameworks. In this work, we propose a novel ap- proach which combines guided anisotropic diffusion with a deep convolutional network and advances the state of the art for guided depth super-resolution. The edge transfer-

Guide

CNN

Diffusion coefficients

Source
ring/enhancing properties of the diffusion are boosted by the contextual reasoning capabilities of modern networks, and a strict adjustment step guarantees perfect adherence to the source image. We achieve unprecedented results in three commonly used benchmarks for guided depth super- resolution. The performance gain compared to other meth-
ods is the largest at larger scales, such as ×32 scaling. Code1 for the proposed method is available to promote re-
producibility of our results.

    1. Introduction
It is a primordial need for visual data analysis to in- crease the resolution of images after they have been cap- tured. In many fields one is faced with images that, for technical reasons, have too low resolutions for the intended purposes, e.g., MRI scans in medical imaging [49], multi- spectral satellite images in Earth observation [23], thermal surveillance images [1] and depth images in robotics [9]. In some cases, an image of much higher resolution is available in a different imaging modality, which can act as a guide for super-resolving the low-resolution source image, by in- jecting the missing high-frequency content. For instance, in Earth observation, the guide is often a panchromatic im- age (hence the term ”pan-sharpening”), whereas in robotics a conventional RGB image is often attached to the same

*Equal contribution.
1 https://github.com/prs- eth/Diffusion- Super- Resolution
Figure 1. We super-resolve a low-resolution depth image by find- ing the equilibrium state of a constrained anisotropic diffusion pro- cess. Learned diffusion coefficients favor smooth depth within ob- jects and suppress diffusion across discontinuities. They are de- rived from the guide with a neural feature extractor that is trained by back-propagating through the diffusion process.

platform as a TOF camera or laser scanner. In this paper, we focus on super-resolving depth images guided by RGB images, but the proposed framework is generic and can be adapted to other sensor combinations, too.
Research into guided super-resolution has a long his- tory [16, 30]. The proposed solutions range from classical, entirely hand-crafted schemes [10] to fully learning-based methods [15], while some recent works have combined the two schools of thought, with promising results [5, 33]. Many classical methods boil down to an image-specific op- timization problem that must be solved at inference time, which often makes them slow and memory-hungry. More- over, they are limited to low-level image properties of the guide, such as color and contrast, and lack the high-level image understanding and contextual reasoning of modern neural networks. On the positive side, by design, they can not overfit the peculiarities of a training set and tend to gen- eralize better. Recent work on guided super-resolution has focused on deep neural networks. Their superior ability to capture latent image structure has greatly advanced the state of the art over traditional, learning-free approaches. Still, these learning-based methods tend to struggle with sharp
discontinuities and often produce blurry edges in the super- resolved depth maps. Moreover, like many deep learning systems, they degrade – often substantially – when applied to images with even slightly different characteristics. Note also that standard feed-forward architectures cannot guar- antee a consistent solution: feeding the source and guiding images through an encoder-decoder structure to obtain the super-resolved target will, by itself, not ensure that down- sampling the target will reproduce the source.
We propose a novel approach for guided depth super- resolution which combines the strengths of optimization- based and deep learning-based super-resolution. In short, our method is a combination of anisotropic diffusion (based on the discretized version of the heat equation) with deep feature learning (based on a convolutional backbone). The diffusion part resembles classical optimization approaches, solved via an iterative diffusion-adjustment loop. Every it- eration consists of (1) an anisotropic diffusion step [2, 4, 24, 31], with diffusion weights driven by the guide in such a way that diffusion (i.e., smoothing) is low across high- contrast boundaries and high within homogeneous regions; and (2) an adjustment step that rescales the depth values such that they exactly match the low-resolution source when downsampled. To harness the unmatched ability of deep learning to extract informative image features, the diffusion weights are not computed from raw brightness values but are set by passing the guide through a (fully) convolutional feature extractor. An overview of the method is depicted in Fig. 1. The technical core of our method is the insight that such a feature extractor can be trained end-to-end to optimally fulfill the requirements of the subsequent opti- mization, by back-propagating gradients through the iter- ations of the diffusion loop. Despite its apparent simplicity, this hybrid approach delivers excellent super-resolution re- sults. In our experiments, it consistently outperforms prior art on three different datasets, across a range of upsampling
factors from ×4 to ×32. In our experiments, we compare it to six recent learning methods as well as five different
learning-free methods. For completeness, we also include a learning-free version of our diffusion-adjustment scheme and show that it outperforms all other learning-free meth- ods. Beyond the empirical performance, our method in- herits several attractive properties from its ingredients: the diffusion-based optimization scheme ensures strict adher- ence to the depth values of the source, crisp edges, and a degree of interpretability; whereas deep learning equips the very local diffusion weights with large-scale context infor- mation, and offers a tractable, constant memory footprint at inference time. In summary, our contributions are:
    1. We develop a hybrid framework for guided super- resolution that combines deep feature learning and anisotropic diffusion in an integrated, end-to-end train- able pipeline;
    2. We provide an implementation of that scheme with constant memory demands, and with inference time that is constant for a given upsampling factor and scales linearly with the number of iterations;
    3. We set a new state of the art for the Middlebury [39], NYUv2 [40] and DIML [20] datasets, for upsampling factors from 4× to 32×, and provide empirical evi- dence that our method indeed guarantees exact consis- tency with the source image.
    2. Related Work
Learning-free methods. Early work on guided super- resolution consisted mostly of optimization methods. Sev- eral such methods [8,26,46] employ random field models to solve this problem. Other traditional methods rely on filters, such as the bilateral filter [47], the guided filter [11], the weighted median filter [25], the weighted mode filter [27], or the Static-Dynamic filter [10]. Liu et al. [24] showed an early, learning-free application of anisotropic diffusion for guided depth enhancement. De Lutio et al. [6] fit a pixel- wise MLP to map the guide to the target and obtain excel- lent results when compared to other learning-free methods, whereas Uezato et al. [42] adapt Deep Image Prior [43] to fuse the guide and the source images.

Learning-based methods. Other avenues have also been explored, such as the auto-regressive model proposed in [44]. More recent architectures [41, 48] have used success- fully applied transformer modules to this problem. With the advent of deep learning in the past decade, several authors have proposed feedforward networks for upsampling depth images. MSG-Net [15] is a U-Net shaped architecture that embeds the source at its lowest scale and learns the resid- ual errors of bicubic interpolations. Kim et al. [18] propose Deformable Kernel Networks (DKN and its efficient imple- mentation FDKN), that learn sparse and spatially-invariant filter kernels. He et al. [13] employ a high-frequency guid- ance module to embed the guide details into the depth map. Wen et al. [45] use convolutional kernels of different sizes while also using data fidelity as a convergence criterion of their iterative refinement.

Hybrid methods. A final group of methods applies deep learning methods within formal frameworks that constrain their solution and improve the methods’ inductive biases for guided super-resolution. Riegler et al. [33] unroll the optimization steps of a first-order primal-dual algo- rithm into a neural network, such that they can train their deep feature extractor in an end-to-end manner. De Lu- tio et al. [5] use the implicit function theorem to propa- gate through a graph-based, MRF-style optimizer, combin- ing deep learning within a framework inspired by traditional
approaches [8, 28, 35]. Work exploring this last family of algorithms has reported promising results from combining the contextual reasoning capabilities of convolutional neu- ral networks (CNNs) with the explicit constraints of formal frameworks. Our work can also be regarded as a member of this group of methods.
    3. Method
We first describe the diffusion-adjustment framework used for guided super-resolution, then we explain how deep learning is integrated into that framework. The latter part allows us to exploit high-level context in the guide image to find optimal coefficients for the diffusion operator.
        3.1. Diffusion-adjustment
Anisotropic diffusion [31] is a form of iterative edge- aware filtering initially proposed with the aim of performing intra-region smoothing while avoiding inter-region smooth- ing. The filtering is done in a way analog to solving a dis-
where the diffusion weights are computed from a separate guide image, the diffusion process transfers edges from the guide to the target image [2, 4, 24]. This is the property that motivates its use in our guided depth super-resolution framework: the diffusion allows us to precisely recover depth discontinuities in the upsampling result.
By itself, diffusion does not take into account the con- straint provided by the source S. I.e., with the machinery introduced so far Yt would approach a constant image with
pixel values µ(Y0) as t → ∞, losing all information. To
tie the output to the source image S, every diffusion step is
followed by an adjustment step that restores compatibility with S. Doing so guarantees that the output of every iter- ation, and therefore also the equilibrium limt→∞ Yt, is a valid upsampling of S.
The adjustment is done by simply re-scaling patches of Yˆ t such that, when downsampled to the source resolution, they exactly match S. More formally, the adjustment step
can be written as
cretized heat equation with anisotropic (i.e., spatially vary- ing) diffusion weights. The idea of computing these diffu- sion weights from a separate (coregistered) guide image has

Yt = Yˆ t · up
	 rt 	
S
down(Yˆ t)

,	(3)
been explored in the context of edge enhancement [24] and semantic segmentation [2, 4].
We are given a source image S ∈ R H × W and a guide G ∈ RH×W ×C, where C = 3 for RGB images or a larger number for deep features. The first step in our approach is to initialize Y0 ∈ RH×W with an upsampled version of S. As we will show later, the exact initialization of Y0 has little
impact on the final result. We can then define a diffusion step as:
yˆp = yp	+ λ · Σ (yn	− yp  ) · c(gp, gn) ,	(1)

Rt
where down denotes a linear downsampling operator, and up denotes nearest neighbor upsampling. rt and Rt are the adjustment ratios at different scales. After this adjustment step, it is guaranteed that the target matches the source at a lower scale, i.e., down(Yt) = S.
An illustration of the method is displayed in Fig. 2. Fur- thermore, we show the evolution during the diffusion pro- cess for a 1D example in Fig. 3. Gradients in the diffused
signal quickly dissipate where gradient in the guide are low,
t	t−1

n∈N p
t−1
t−1
but diffusion (almost) stops at edges of the guide. Consis- tency with the source S is preserved throughout the process.
where yp denotes the pixel value of Yt at location p (and similarly for gp). N p denotes the 4-neighborhood of pixel
p. Note that this construction connects all pixels in the im-
age into a (planar) graph. λ is a strictly positive hyper- parameter that regulates the update and ensures stability.
When using N p connectivity it should be set to λ < 1 . The
        3.2. Deep Feature-Guided Anisotropic Diffusion
Recent work has shown that the context features ex- tracted by with CNNs over large receptive fields can mas- sively boost super-resolution based on low-order graphs [5].
4	4	A main message of our paper is that this idea is even more
function c : (RC, RC) → (0, 1) produces diffusion coeffi- cients for neighboring pairs of pixels based on their values
in the guide. Following [31], we use
powerful when combined with diffusion on the graph.
Let F : RH×W ×C1 → RH×W ×C2 be a neural fea- ture extractor. In our experiments, F is a U-Net [34] with
c(gp, gn) =	κ
(2)
ResNet-50 [12] backbone pretrained on ImageNet [7] and C2 = 64, but any other neural architecture could be used, as long as the spatial dimensions of the output match those
where κ is a hyperparameter that regulates the sensitivity to gradients in G. Note that c is symmetrical, c(gp, gn) = c(gn, gp). Traditional (non-guided) anisotropic diffusion is a special case of the formulation above where G = Yt−1.
When applied to a single image, anisotropic diffusion has edge-enhancing properties [31]. In guided diffusion,
of the input. While F could be applied directly to the RGB guide G, we concatenate the upsampled source up(S) as a fourth channel to support object separation based on coarse depth cues, so C1 = 4.
Previous work that connected guided anisotropic diffu- sion with deep learning [2, 4] was restricted to using it only

RGB version	Diffusion-adjustment loop
Figure 2. The diffusion step performs anisotropic diffusion on the depth image using diffusion weights given by a CNN. The adjustment step ensures that the diffused result matches the source image at its original resolution. Gradients from the loss function can be back- propagated back to the CNN, making the system end-to-end trainable.



Figure 3. Diffusion diminishes gradients in the target signal (high- lighted in red). Strong gradients in the guide signal (marked in green) impede diffusion and are thus transferred to the target. Ad- ditionally, the target is constrained to adhere to the low-resolution source (depicted in blue).

as a post-processing step at inference time, due to exces- sively high memory requirements, and a risk of vanish- ing/exploding gradients caused by the iterative nature of the algorithm. We found that it is in fact possible to propagate gradients through the diffusion process with the following scheme:
            1. We compute the c(gp, gn) only once before diffusion starts and freeze them. This is in contrast to earlier attempts that diffused the guide G alongside Y and had to update the c(gp, gn) at every iteration [2, 4];
            2. We back-propagate gradients only through the last
Ngrad  diffusion-adjustment  iterations  (in  practice
Ngrad ≈ 103). I.e., during training the feature extrac- tor receives training signals from the later stages of the diffusion process.
With these modifications, we can effectively back- propagate gradients all the way to the feature extractor F and train the entire pipeline end-to-end. Furthermore, note that κ in Eq. (2) is also trainable, and therefore does not need to be chosen manually.
At training time we use a random number of iterations without gradient before computing iterations with gradient. We define Npre as the maximum number of iterations with- out gradient updates at training time and Ngrad as the num- ber of iterations with gradients. We found that this random- ization helps to speed up the convergence at inference time (see supplementary material). For inference we use a con- stant N = Npre + Ngrad iterations. The rationale behind us- ing the last Ngrad iterations for the computation of gradients is that we are interested in the steady-state equilibrium of the system, and therefore need a high number of iterations. We study the effect of Npre and Ngrad and the associated computational costs in Section 4, and find that there is little gain for Ngrad > 1024, a setting at which the system can still be trained on a single GPU. Finally, we point out that while the algorithm needs a relatively large number of iterations (we find 8000 to be a suitable number), the diffusion and adjustment operators are extremely lightweight and paral- lelizable, and as a result, our algorithm is still faster than other optimization approaches.

    4. Experiments
We evaluate our method on three different RGB-D datasets commonly used for super-resolution, namely Mid- dlebury 2005-2014 [14, 36–39], NYUv2 [40], and DIML
[3, 19–21]. We closely follow the setup of [5], including the

×4
MSE / MAE
×8
MSE / MAE
×16
MSE / MAE
×32
MSE / MAE


×4
MSE / MAE
×8
MSE / MAE
×16
MSE / MAE
×32
MSE / MAE


Middlebury





Middlebury


MSG
4.13 / 0.22
10.5 / 0.43
34.2 / 1.06
92.9 / 2.55

Bicubic
13.3 / 0.55
30.0 / 1.10
68.5 / 2.13
143 / 3.87
DKN
4.29 / 0.18
11.2 / 0.38
47.6 / 1.42
119 / 3.35

GF
33.3 / 1.27
40.5 / 1.49
67.4 / 2.21
134 / 3.82
FDKN
3.60 / 0.16
10.4 / 0.37
38.5 / 1.18
112 / 3.23

SD
24.9 / 0.46
82.5 / 0.86
511 / 1.73
4062 / 3.37
PMBA
4.72 / 0.25
9.48 / 0.38
30.6 / 0.89
175 / 5.15

PixT
39.8 / 0.79
32.7 / 0.82
41.5 / 1.24
107 / 2.71
FDSR
7.72 / 0.35
23.2 / 0.69
55.4 / 1.51
179 / 4.11

LGR†
14.8 / 0.42
68.3 / 0.83
297 / 1.69
897 / 3.31
LGR
3.04 / 0.13
7.26 / 0.24
24.7 / 0.67
63.4 / 1.75

DADA†
11.1 / 0.40
18.9 / 0.70
36.7 / 1.30
84.1 / 2.58
 DADA	2.52 / 0.11	5.63 / 0.20	15.6 / 0.47	47.6 / 1.35 	NYUv2













NYUv2



Bicubic
31.9 / 1.59
89.9 / 3.17
242 / 6.01
588 / 10.5
MSG
6.85 / 0.81
24.1 / 1.66
84.5 / 3.35
262 / 6.70

GF
114 / 3.91
142 / 4.47
249 / 6.34
556 / 10.4
DKN
11.4 / 1.03
29.8 / 1.82
115 / 4.01
364 / 8.33

SD
36.0 / 1.31
105 / 2.57
533 / 5.07
3246 / 10.4
FDKN
8.07 / 0.85
29.9 / 1.80
113 / 3.95
365 / 8.39

PixT
112 / 3.61
122 / 3.86
219 / 5.40
759 / 11.6
PMBA
10.8 / 0.93
31.5 / 1.79
84.9 / 3.26
449 / 9.11

LGR†
19.0 / 1.11
68.4 / 2.30
264 / 4.56
790 / 9.31
FDSR
10.5 / 0.94
35.4 / 1.96
179 / 4.68
771 / 11.8

DADA†
18.1 / 1.05
49.9 / 2.05
125 / 3.88
328 / 7.50

LGR
6.45 / 0.73
19.6 / 1.42
67.5 / 2.90
253 / 6.50
DADA
4.87 / 0.64
17.1 / 1.33
59.2 / 2.65
223 / 5.76
DIML

MSG
1.73 / 0.22
4.13 / 0.40
13.0 / 0.93
55.0 / 2.56
DKN
3.47 / 0.33
5.47 / 0.45
19.3 / 1.20
91.5 / 3.75
FDKN
2.2 / 0.23
5.95 / 0.47
20.8 / 1.24
89.9 / 3.75
PMBA
3.05 / 0.31
5.87 / 0.47
13.8 / 0.87
55.1 / 2.30
FDSR
2.75 / 0.29
8.40 / 0.66
32.9 / 1.66
124 / 4.39



Table 1. Performance comparison of learning-based methods in terms of MSE (in cm2) and MAE (in cm). DADA consistently outperforms all other methods, especially at large scaling factors. We report the variability of our method in the supplementary ma- terial.

train/validation/test splits, data prepossessing and usage of evaluation metrics.
Middlebury [14, 36–39] consists of 50 photogrammet- rically created high-resolution depth maps and their associ- ated RGB images from the years from 2005 to 2014. Five samples are reserved for validation and testing each, while the rest is used for training. The depth maps contain data gaps, which are also present in the source images. This dataset provides the most accurate ground truth among the considered datasets.
NYUv2 [40] was captured with a Microsoft Kinect depth camera. It consists of a total of 1449 RGB-D images, from which 849 are used for training, and 300 each for validation and testing.
DIML [3, 19–21] contains 2 million RGB-D samples, from which we only use the high quality indoor samples that were acquired with a Microsoft Kinect depth camera. 1440 are training samples, 169 are validation samples and 503 are test samples.
        4.1. Experimental Setup
We compare our deep anisotropic diffusion-adjustment network (DADA) against a broad, representative range of
DIML

Bicubic GF
10.4 / 0.63	28.6 / 1.32	73.2 / 2.68	187 / 5.37
25.6 / 1.45	34.1 / 1.77	66.3 / 2.74	165 / 5.18
SD
10.5 / 0.40
44.9 / 0.83
411 / 1.91
5905 / 4.45
PixT
20.7 / 1.15
23.0 / 1.26
39.3 / 1.78
141 / 4.19
LGR†
7.02 / 0.35
15.2 / 0.67
133 / 1.72
815 / 3.98
DADA†
4.40 / 0.28
9.35 / 0.51
21.3 / 1.15
72.6 / 3.02



adjustment scheme, without feature learning, outperforms other learning-free methods. † marks learning-free variants of hybrid methods.

guided super-resolution methods, both learning-based and learning-free. We also include simple non-guided bicu- bic upsampling (Bicubic) [17] as a baseline and sanity check. The learning-based methods we consider are MSG- Net (MSG) [15], Deformable Kernel Network (DKN) [18], Fast Deformable Kernel Network (FDKN) [18], Fast Depth Super-Resolution (FDSR) [13], PMBANet (PMBA) [48] and Learned Graph Regularizer (LGR) [5]. Learning-free methods in our evaluation are the guided filtering (GF) [11], Static/Dynamic filtering (SD) [10], Pixtransform (PixT) [6], and a version of LGR [5] that is based on raw RGB values in the guide, rather than learned features. In much the same way, we also run our diffusion-adjustment scheme with dif- fusion weights derived from raw RGB values. For upsam-
pling factors of ×4 to ×16, the scores are taken directly from [5], for scale ×32 we have generated all results our- selves, following the setup described in [5] and using their
open-source code base. As error metrics, we show both the mean squared error (MSE) and the mean absolute error (MAE) of the predicted depth images.
Our experiments were conducted using PyTorch [29] and we train all methods, including our own, with the L1 loss function. Further details regarding hyper-parameters for training will be provided in the supplementary material and code to ensure reproducibility. For the learning-free variant of DADA we set κ = 0.03 (see Eq. (2)).


1.0




0.8




0.6




0.4




0.2

Guide	RGB coefficients	Deep coefficients
0.0
Figure 4. Diffusion coefficients calculated from raw RGB values and from deep features. The CNN focuses on object discontinu- ities rather than texture edges. Note the text on the box, top left.


        4.2. Results
Table 1 shows quantitative results on all three datasets for the learned methods, while Tab. 2 shows the results for the non-learned ones. Our proposed method obtains the best performance in terms of both MSE and MAE on all three datasets, and across the full range of upsampling fac-
tors, with one single exception (at factor ×16 on Middle- bury, it is 2nd-best). The results underline the general trend
towards learned, high-level feature representations: gener- ally speaking, the learned methods clearly outperform the learning-free ones. In particular, for LGR as well as for our DADA, the versions with learned features easily beat their RGB-based counterparts. Having said that, we note that many learning-based methods seem to struggle with large- scale differences between the source and the guide (respec- tively, the target): the gap between learned and non-learned methods shrinks with increasing upsampling factors, and in fact, the performance of several dedicated super-resolution methods (learning-free ones, but also learned ones) falls be- low that of na¨ıve bicubic upsampling.
RMSE curves for different scaling factors using the Mid- dlebury dataset can be seen in Fig. 7. The plots for the other datasets follow the same trends and will be included in the supplementary material. As seen in these results, the
performances of several of the considered methods degrade severely for larger scaling factors. In fact, for scale ×32 many of the methods perform worse than simple bicubic interpolation. The results also show that for the lowest scal- ing factor (×4) many learned methods achieve similar per-
formance, suggesting that guided super-resolution for mod-
erate scale factors is reaching saturation, at least w.r.t. the currently available training and test data.
Figure 5 contains results from several of the compared methods on all three datasets with scaling factors of ×16 and ×32. The residual images make it clear that DADA is superior to all other methods, including LGR – the previous

Table 3. Cross-dataset generalization performance. All learned methods were trained on NYUv2, then tested on DIML and Mid- dlebury, with scaling factors of ×8 and ×32. Errors are in cm2 for the MSE and in cm for the MAE. The low-resolution MSE‡, i.e., after downsampling the predicted target, indicates inconsistency with the forward model (linear downsampling).

best. Strict constraint to the source is especially advanta- geous in regions with no depth discontinuities. DADA is nevertheless able to produce sharp edges where necessary. Encircled depth discontinuities seem to be the most chal- lenging cases for all methods. Although DADA still out- performs other methods, those situations still offer room for improvement.
Our quantitative and qualitative results both show the superiority of the results DADA yields when compared to all other methods. Our results also lend further sup- port to the findings of LGR [5] that theoretically founded, optimization-based methods still offer excellent perfor- mance within-domain and especially out-of-domain if (and only if) they are integrated with the large receptive field and superior perceptual grouping abilities of modern, deep fea- ture extractors. Having said that, we find that DADA sur- passes LGR in terms of both reconstruction error and com- putational cost (memory consumption as well as runtime), especially at high upsampling factors.
In Figure 4 we show a pair of diffusion coefficients de- rived from deep features and contrast them with coefficients that are directly derived from the RGB values. The learned coefficients are more robust to printed text and logos, image noise, and textured surfaces, whereas the unlearned ones are overly sensitive to those.
Experience suggests that deep learning methods tend to (over-)fit to dataset-specific characteristics and generalize poorly from one dataset to another. For LGR, which like our method explicitly enforces consistency between the (down- sampled) target and the source, the authors in fact demon- strate improved generalization compared to purely learned frameworks. They attribute this behavior to the consistency constraint, which is data-independent and therefore not af- fected by domain gaps [5]. We have also explored the gen-





























Guide	Source	PixT	MSG	FDKN	PMBA	FDSR	LGR	DADA	DADA pred.	GT
Figure 5. Error maps for different guided super-resolution methods. Blue denotes under-estimated depth, red denotes over-estimation. All errors maps in a row have the same color scale. The last two columns juxtapose our predictions and the ground truth.

eralization of our method across datasets, and report the re- sults in Table 3. For these experiments, we train on the NYUv2 dataset for a super-resolution factor of ×8 and ×32, and test the resulting network on Middlebury and DIML. Also for this task, our method outperforms all competitors,
including LGR, which ranks second.
        4.3. Ablations
In this section, we experimentally study the effects of different parameters and modules on the results of our super-resolution method. We run these experiments on the Middlebury dataset with upsampling factor ×8, which we found to be a representative setting. The memory require-
ments reported below always refer to a training batch con- sisting of 8 images of size 256 × 256.

Initialization. Our default initialization scheme for the super-resolved image (bicubic upsampling of the guide) and for the feature extractor (Imagenet pretraining) are not the only options. Compared to the base case (MSE = 5.67, MAE = 0.199), we explore the two simplifications: (1) random initialization of the ResNet-50 backbone weights instead of ImageNet pretraining (which yields (MSE = 5.92, MAE = 0.211); and (2) constant initialization of Y0 rather than bicubic interpolation of S (which yields (MSE = 5.64, MAE = 0.200). In both cases, we ob-
serve only tiny differences w.r.t. the base case. With re- gard to the backbone weights, this suggests that the avail- able amount of training data is sufficient to learn a suitable feature representation. For the initialization of the target image Y0, it is not surprising that this has almost no effect on the result. It is easy to see that the first diffusion step will not alter the constant input and that the output Y1 of the first adjustment step will then be the nearest-neighbor upsampling of S. Meaning that after a single iteration of the diffusion-adjustment loop, the constant initialization has largely caught up with the seemingly more sophisticated bicubic one (and in fronto-parallel areas perhaps even has overtaken it).



Number of training iterations with gradient. As ex- plained above, due to memory constraints we limit the num- ber of diffusion iterations that propagate gradients into the feature extractor. In Figure 6a, we explore how this af- fects feature learning. Memory requirements increase with Ngrad, ranging from 11 GB for Ngrad = 32 up to 23 GB for Ngrad = 1024. We find that larger values of Ngrad gener- ally lead to better performance, likely due to the gradients being more representative of the equilibrium point of the diffusion-adjustment process.
2.50

2.48

2.46

2.44

2.42

2.46
20

2.44
15

2.42
10

3.0
2.9
2.8
2.7
2.6


200


150


100
2.40

2.38

2.36


5


0
32	64	128	256	512	1024
Ngrad

2.40


2.38






0	1000	2000	4000	8000	16000
Npre
2.5
2.4
2.3


50


0
0  2000  4000  6000  8000 10000 12000 14000 16000
N
    (a) Number Ngrad of iterations with gradient prop- agation and memory consumption.
    (b) Number Npre of iterations without gradient propagation.
    (c) Number of iterations used at inference time and average computing time per sample.
Figure 6. Plots for ablation experiments that explore the effects of different numbers of iterations in the diffusion-adjustment loop. We generally see an increase in performance for larger numbers of iterations, albeit at higher computational cost.





12


10


8


6


4
inference time. In Figure 6c we explore how the number of iterations at test time impacts the results. Once more, higher iteration counts N lead to better results, as the process con- verges further towards the equilibrium point limt→∞ Yt. We see little improvement for N > 8000. As computing time (also shown in the graph) scales linearly with N , we see no reason to go beyond N = 8000.

2









17.5

15.0

12.5

10.0

7.5

5.0

2.5



4	8	16	32
Upsampling factor
    (a) RMSE (cm) of learning-based methods












4	8	16	32
Upsampling factor
    (b) RMSE (cm) of learning-free methods
    5. Conclusion
We have presented DADA, a simple yet extremely effec- tive approach to guided super-resolution of depth images. The method links guided anisotropic diffusion with deep learning. The diffusion part offers a computationally ef- ficient optimization framework to optimally align the pre- diction with the individual input images at test time and explicitly constrains the results to comply with the source image. The deep learning-based feature extractor harnesses the representation power of CNNs to optimally inform the diffusion process, by injecting context cues collected over large receptive fields into the diffusion weights. In our ex- periments, DADA reaches top performance on three pop- ular datasets and outperforms the prior state-of-the-art, in-
Figure 7. RMSE values at different upsampling factors, for the Middlebury dataset. Our method consistently achieves the lowest error.


Number of training iterations without gradient. A main hyper-parameter of the (asymptotic) diffusion process is the total number of iterations. For a fixed setting of Ngrad this is, in our scheme, governed by the number of preceding gradient-free iterations, Npre. We vary that value and once again we find that, as expected, larger values generally lead to better results, likely because the gradients are computed from a state closer to the equilibrium point. We see little further gain after Npre = 8000.

Number of iterations at test time. Once trained, we are free to decide for how long we run the diffusion process at
cluding learning-based, optimization-based as well as hy- brid methods. We also found that DADA is fairly robust in terms of engineering details: it is barely affected by the choice of pretraining and initialization scheme and is rela- tively tolerant against hyper-parameter changes.
An interesting direction for future work will be to formu- late the diffusion-adjustment iteration as an ordinary differ- ential equation. This could potentially make it possible to back-propagate through the entire diffusion process with the adjoint sensitivity method [32], rather than unrolling a lim- ited number of iterations. At a conceptual level, we hope that our work motivates further research about the combi- nation of classical optimization-based computer vision and modern, deep learning-based image representations. In line with others [5,33], our work suggests that such hybrid meth- ods hold great potential, not only for super-resolution, but also for other low-level vision tasks, and perhaps beyond.
References
    [1] Feras Almasri and Olivier Debeir. RGB guided thermal super-resolution enhancement. Int’l Conf on Cloud Com- puting Technologies and Applications, 2018. 1
    [2] Rodrigo Caye Daudt, Bertrand Le Saux, Alexandre Boulch, and Yann Gousseau. Guided anisotropic diffusion and itera- tive learning for weakly supervised change detection. CVPR Workshops, 2019. 2, 3, 4
    [3] Jaehoon Cho, Dongbo Min, Youngjung Kim, and Kwanghoon Sohn. Deep monocular depth estimation leveraging a large-scale outdoor stereo dataset. Expert Systems with Applications, 178:114877, 2021. 4, 5
    [4] Rodrigo Caye Daudt, Bertrand Le Saux, Alexandre Boulch, and Yann Gousseau. Weakly supervised change detection using guided anisotropic diffusion. Machine Learning, pages 1–27, 2021. 2, 3, 4
    [5] Riccardo de Lutio, Alexander Becker, Stefano D’Aronco, Stefania Russo, Jan D Wegner, and Konrad Schindler. Learn- ing graph regularisation for guided super-resolution. CVPR, 2022. 1, 2, 3, 4, 5, 6, 8, 11
    [6] Riccardo de Lutio, Stefano D’Aronco, Jan Dirk Wegner, and Konrad Schindler. Guided super-resolution as pixel-to-pixel transformation. ICCV, 2019. 2, 5, 11
    [7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. CVPR, 2009. 3
    [8] James Diebel and Sebastian Thrun. An application of Markov random fields to range sensing. NIPS, 2005. 2, 3
    [9] Ivan Eichhardt, Dmitry Chetverikov, and Zsolt Janko. Image-guided ToF depth upsampling: a survey. Machine Vi- sion and Applications, 28(3):267–282, 2017. 1
    [10] Bumsub Ham, Minsu Cho, and Jean Ponce. Robust guided image filtering using nonconvex potentials. IEEE TPAMI, 40(1):192–207, 2017. 1, 2, 5
    [11] Kaiming He, Jian Sun, and Xiaoou Tang. Guided image fil- tering. IEEE TPAMI, 35(6):1397–1409, 2012. 2, 5
    [12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. CVPR, 2016. 3, 11
    [13] Lingzhi He, Hongguang Zhu, Feng Li, Huihui Bai, Runmin Cong, Chunjie Zhang, Chunyu Lin, Meiqin Liu, and Yao Zhao. Towards fast and accurate real-world depth super- resolution: Benchmark dataset and baseline. CVPR, 2021. 2, 5, 13
    [14] Heiko Hirschmu¨ller and Daniel Scharstein. Evaluation of cost functions for stereo matching. CVPR, 2007. 4, 5
    [15] Tak-Wai Hui, Chen Change Loy, and Xiaoou Tang. Depth map super-resolution by deep multi-scale guidance. ECCV, 2016. 1, 2, 5
    [16] David Izraelevitz. Model-based multispectral sharpening.
Proceedings of SPIE, 1994. 1
    [17] Robert Keys. Cubic convolution interpolation for digital im- age processing. IEEE T Acoustics, Speech, and Signal pro- cessing, 29(6):1153–1160, 1981. 5
    [18] Beomjun Kim, Jean Ponce, and Bumsub Ham. Deformable kernel networks for joint image filtering. IJCV, 129(2):579– 600, 2021. 2, 5
    [19] Sunok Kim, Dongbo Min, Bumsub Ham, Seungryong Kim, and Kwanghoon Sohn. Deep stereo confidence prediction for depth estimation. ICIP, 2017. 4, 5
    [20] Youngjung Kim, Bumsub Ham, Changjae Oh, and Kwanghoon Sohn. Structure selective depth superresolution for RGB-D cameras. IEEE TIP, 25(11):5227–5238, 2016. 2,
4, 5
    [21] Youngjung Kim, Hyungjoo Jung, Dongbo Min, and Kwanghoon Sohn. Deep monocular depth estimation via integration of global and local predictions. IEEE TIP, 27(8):4131–4144, 2018. 4, 5
    [22] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 11
    [23] Charis Lanaras, Jose´ Bioucas-Dias, Silvano Galliani, Em- manuel Baltsavias, and Konrad Schindler. Super-resolution of Sentinel-2 images: Learning a globally applicable deep neural network. ISPRS J Photogrammetry and Remote Sens- ing, 146:305–319, 2018. 1
    [24] Junyi Liu and Xiaojin Gong. Guided depth enhancement via anisotropic diffusion. Pacific-Rim Conf on Multimedia, 2013. 2, 3
    [25] Ziyang Ma, Kaiming He, Yichen Wei, Jian Sun, and En- hua Wu. Constant time weighted median filtering for stereo matching and beyond. ICCV, 2013. 2
    [26] Oisin Mac Aodha, Neill D. F. Campbell, Arun Nair, and Gabriel J. Brostow. Patch based synthesis for single depth image super-resolution. ECCV, 2012. 2
    [27] Dongbo Min, Jiangbo Lu, and Minh N Do. Depth video enhancement based on weighted mode filtering. IEEE TIP, 21(3):1176–1190, 2011. 2
    [28] Min-Gyu Park and Kuk-Jin Yoon. As-planar-as-possible depth map estimation. Computer Vision and Image Under- standing, 181:50–59, 2019. 3
    [29] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zem- ing Lin, Natalia Gimelshein, Luca Antiga, et al. PyTorch: An imperative style, high-performance deep learning library.
NeurIPS, 2019. 5
    [30] Tim J. Patterson, Michael E. Bullock, and Alan D. Wada. Multispectral band sharpening using pseudoinverse estima- tion and fuzzy reasoning. Proceedings of SPIE, 1992. 1
    [31] Pietro Perona and Jitendra Malik. Scale-space and edge de- tection using anisotropic diffusion. IEEE TPAMI, 12(7):629– 639, 1990. 2, 3
    [32] Lev S. Pontryagin. Mathematical theory of optimal pro- cesses. CRC Press, 1987. 8
    [33] Gernot Riegler, David Ferstl, Matthias Ru¨ther, and Horst Bischof. A deep primal-dual network for guided depth super- resolution. BMVC, 2016. 1, 2, 8
    [34] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation.
MICCAI, 2015. 3, 11
    [35] Mattia Rossi, Mireille El Gheche, Andreas Kuhn, and Pas- cal Frossard. Joint graph-based depth refinement and nor- mal estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12154– 12163, 2020. 3
    [36] Daniel Scharstein, Heiko Hirschmu¨ller, York Kitajima, Greg Krathwohl, Nera Nesˇic´, Xi Wang, and Porter West- ling. High-resolution stereo datasets with subpixel-accurate ground truth. GCPR, 2014. 4, 5
    [37] Daniel Scharstein and Chris Pal. Learning conditional ran- dom fields for stereo. CVPR, 2007. 4, 5
    [38] Daniel Scharstein and Richard Szeliski. High-accuracy stereo depth maps using structured light. CVPR, 2001. 4, 5
    [39] Daniel Scharstein, Richard Szeliski, and Ramin Zabih. A taxonomy and evaluation of dense two-frame stereo corre- spondence algorithms. IJCV, 47(1):7–42, 2002. 2, 4, 5
    [40] Evan Shelhamer, Jonathan Long, and Trevor Darrell. Fully convolutional networks for semantic segmentation. IEEE TPAMI, 2017. 2, 4, 5
    [41] Xibin Song, Yuchao Dai, Dingfu Zhou, Liu Liu, Wei Li, Hongdong Li, and Ruigang Yang. Channel attention based iterative residual learning for depth map super-resolution.
CVPR, 2020. 2
    [42] Tatsumi Uezato, Danfeng Hong, Naoto Yokoya, and Wei He. Guided deep decoder: Unsupervised image pair fusion.
ECCV, 2020. 2
    [43] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky.
Deep image prior. CVPR, 2018. 2
    [44] Jin Wang, Wei Xu, Jian-Feng Cai, Qing Zhu, Yunhui Shi, and Baocai Yin. Multi-direction dictionary learning based depth map super-resolution with autoregressive modeling. IEEE T Multimedia, 22(6):1470–1484, 2019. 2
    [45] Yang Wen, Bin Sheng, Ping Li, Weiyao Lin, and David Da- gan Feng. Deep color guided coarse-to-fine convolutional network cascade for depth image super-resolution. IEEE TIP, 28(2):994–1006, 2018. 2
    [46] Jun Xie, Rogerio Schmidt Feris, and Ming-Ting Sun. Edge- guided single depth image super resolution. IEEE TIP, 25(1):428–438, 2015. 2
    [47] Qingxiong Yang, Ruigang Yang, James Davis, and David Niste´r.  Spatial-depth super resolution for range images.
CVPR, 2007. 2
    [48] Xinchen Ye, Baoli Sun, Zhihui Wang, Jingyu Yang, Rui Xu, Haojie Li, and Baopu Li. PMBANet: Progressive multi-branch aggregation network for scene depth super- resolution. IEEE TIP, 2020. 2, 5
    [49] Yongqin Zhang, Feng Shi, Jian Cheng, Li Wang, Pew-Thian Yap, and Dinggang Shen. Longitudinally guided super- resolution of neonatal brain magnetic resonance images. IEEE T Cybernetics, 49(2):662–674, 2018. 1
Supplementary Material
Guided Depth Super-Resolution by Deep Anisotropic Diffusion

    A. Videos
Scale	x4	x8	x16	x32

Videos depicting the diffusing process (Yt and residuals) are included with the supplementary materials. The exam-
MSE [cm2]
MAE [cm]
MAPE [%]
2.52±0.04 5.63±0.09 15.6±0.10 47.6±0.50
0.11±0.00 0.20±0.00 0.47±0.00 1.35±0.01
0.04±0.00 0.07±0.00 0.17±0.00 0.46±0.00
ples are from the Middlebury dataset, with a scaling factor
VV [%]
0.06±0.00 0.12±0.00 0.30±0.00 0.91±0.01
of ×32.
EE [%]
0.82±0.02 1.43±0.01 2.92±0.02 8.09±0.03

    B. Training set up
We train our method with an Adam optimizer [22] with β1 = 0.9, β2 = 0.999, a learning of 10−3, and weight de- cay parameter set to 10−5. Furthermore, we clip gradients to a maximum norm of 0.01 for stability during training. We find that DADA reaches convergence after 4500, 550, and 300 epochs for the datasets of Middlebury, NYUv2, and DIML respectively. During training, we apply data aug- mentation, which includes horizontal flips, random crop- ping, and rotating the samples by up to 15 degrees. We nor- malize all depth maps with their respective dataset-specific standard deviation. The Mean is not subtracted since the adjustment step assumes positive values.
The feature extractor is a U-Net [34] with ResNet- 50 [12] backbone that operates on a ×2 upsampled guide. Subsequently, the produced feature maps are downsampled again to the original spatial resolution. We ablate this de- sign choice in Appendix G.


    C. Experimental set up
Our experiments and baselines closely follow the setup described in [5] (supplementary material) and compare with mostly the same baselines. All methods are trained with the
default settings of the Adam optimizer and a learning rate of 10−4 (except PMBA: 10−3, FDSR: 5 × 10−4). We train all learned models until convergence, which is reached after 2500, 250, and 150 epochs on the datasets of Middlebury,
NYUv2, and DIML respectively. We reduce the learning rate by a factor of 0.9 every 100, 10 and 6 epochs for Mid- dlebury, NYUv2, and DIML, respectively. In order to limit the GPU memory consumption to a manageable level, we had to reduce the patch size for some baselines from 2562 to 1282 for PMBA x4 even to 642. For the guided filter, we use a radius of 8, and for the SD Filter, we use the following hyperparameter configuration: λ = 0.1, σg = 60, σu = 30. For Pixtransform, we use the standard hyperparameters re- ported in their paper [6].




MSE [cm2]
MAE [cm]
1.30±0.02 2.87±0.06 7.75±0.12 38.6±0.80
0.17±0.00 0.27±0.00 0.60±0.01 1.90±0.02
MAPE [%]
0.06±0.00 0.10±0.00 0.21±0.00 0.68±0.01
VV [%]
0.03±0.00 0.07±0.00 0.18±0.00 0.85±0.01
EE [%]
2.44±0.01 3.75±0.03 7.39±0.08 19.5±0.19
Table A1. Variability of metrics on the Middlebury, NYUv2, and DIML datasets in the top, middle, and bottom sections, respec- tively.

    D. Stability & Statistics
We also repeated the training using 4 additional (5 total) different random seeds to determine the epistemic uncer- tainty/stability of our method. We observe little variability across runs that consistently surpass previous results, see Tab. A1.
    E. GPU memory consumption
In Figure A1 we show the GPU memory consumption of our method at training time (a) and at inference (b) and con- trast them to the LGR approach, which is our closest com-
petitor and is also a hybrid method. DADA requires ∼23 GB at training time and ∼4 GB at inference time, regard- less of the upsampling factor, while LGR uses up to ∼64 GB for training and ∼35 GB for testing.
    F. Inference time
We measure a mean inference time for the proposed method of ∼100 ms per sample when using a batch size of 32 on an NVIDIA GeForce RTX 3090, with all other pa- rameters equal to the canonical ones previously described.
We note that the inference time is invariant w.r.t. the upsam- pling factor. For LGR we found it to increase with the scal- ing factor: 50 ms, 84 ms, 240 ms, and 910 ms for ×4, ×8,
×16, and ×32, respectively. In contrast, the feedforward


60

50

40
U-Net U-Net U-Net U-Net U-Net U-Net FPN DL3+ RN18 RN34 RN50 ENB0 ENB1 ENB2 RN50 RN50

30

20	Table A2. Middlebury, scale x8. DADA is invariant to the encoder
10	depth and fairly invariant to the choice of model architecture.
0
4	8	16	32
Ngrad
        (a) Memory requirement during training
35

30



Middlebury

25

20

15

10

5

0
4	8	16	32
Ngrad
        (b) Memory requirement during testing

Figure A1. Memory requirements for a batch of 8 samples for DADA and LGR. Constant memory requirements are an advantage at higher scaling factors.


methods are a lot faster – most of them are below 20 ms.

    G. Further ablations
Upsampling of the guide. We explore two different ways of employing a U-Net feature extractor: Our default case where we upsample the guide by ×2 and we downsample the obtained feature maps afterward and a case without up- and downsampling modifications. Tab. A3, shows that this modification brings an improvement in the settings of ×4 to
×16, while a slight weakening effect is observed for ×32.
Our intuition is that upsampling helps to produce sharp and
accurately localized edges, which is especially beneficial for smaller scaling factors. Inversely, we speculate that for
×32 global context plays a bigger role, and hence, upsam- pling the guide decreases the receptive field of the U-Net,
which in turn deteriorates the method’s performance.


Randomization of iterations. In Fig. A2 we show the ef- fect of randomizing the number of iterations without gradi- ent at training time (as proposed) against choosing it to be a constant Ngrad = 8000. At test time, it seems that both strategies eventually converge to an equally performant so- lution. However, we note that our proposed way of training leads to faster convergence in the diffusion-adjustment iter- ations.

NYUv2


DIML


Table A3. Ablation: method as proposed vs. method which skips upsampling before and downsampling after the feature extractor. Errors are in cm2 (MSE) and in cm (MAE). Up-/downsampling appears to have a positive effect for lower scales and a neutral or slightly negative effect for very large scales.

3.2


3.0


2.8


2.6


2.4

250	500	1000	2000	4000	8000  16000 32000 64000
N

Figure A2. Randomizing the number of iterations without gradient during training speeds up convergence at inference time.


Feature Extractor. We explored additional feature ex- tractors by replacing the base case (U-Net with ResNet-50) with two more different ResNet depths (18 and 34) and Ef- ficientNets (B0, B1, B2). We also tested different feature extractors (FPN, DeepLabv3+) with the ResNet-50 back- bone. We show the results in Tab. A2. DADA is robust to changes in the exact architecture of the feature extractor.
    H. Additional metrics
We concur that other metrics can provide additional in- sight into the method’s performance and facilitate future comparisons. We computed the mean absolute percentage
Middlebury	NYUv2	DIML
in [%] MAPE/VE/EE MAPE/VE/EE MAPE/VE/EE
upsampling factors, the advantages of DADA become more apparent: Shapes are represented more accurately and edges
MSG DKN
0.8 / 1.7 / 14.7 1.9 / 3.0 / 49.8 0.9 / 1.2 / 24.8
1.2 / 2.7 / 21.9 2.4 / 4.4 / 55.8 1.4 / 2.2 / 32.3
are sharper.
FDKN 1.2 / 2.6 / 21.7 2.4 / 4.5 / 55.7 1.4 / 2.2 / 34.3
PMBA 1.9 / 4.1 / 35.7 2.7 / 5.2 / 57.4 0.8 / 1.2 / 25.0
FDSR LGR DADA
1.6 / 3.7 / 21.6 3.4 / 7.2 / 63.4 1.6 / 2.5 / 36.3
0.6 / 1.2 / 12.8 1.8 / 2.7 / 49.0 0.9 / 1.3 / 26.7
0.5 / 0.9 / 8.1 1.6 / 2.4 / 44.3 0.7 / 0.9 / 19.7
Table A4. Middlebury, scale x32, all numbers are in [%].

2
5
6
5
3
1

6
5









3
8
6
4

2
0.5

6
4











3


3


1


1










2
1
3
4






6
3
3
4


3
2
6
7

3
3
1
1

9
6
6
7


3
3
12
9

2
2
0.5
0.5

6
6
6
4.5


3
3
8
3

2
2
0.5
0.5

6
6
4
1.5


		

Figure A3. Numerical example of the adjustment step.

error (MAPE), value errors (VE), and edge errors (EE) as described in [13]. The advantage of DADA w.r.t. current methods remains clear, see Tab. A4.
    I. Main results as curves
For improved interpretation, the RMSE numbers from Tables 1 and 2 from the main paper are here also displayed as curves. These plots can be seen in Figure A4.
    J. Intuition behind the adjustment step
The values of rt and Rt are computed precisely so that down(Yt) matches S. The values in rt are per-patch adjust- ment coefficients. A numerical example of these operations is shown in Fig. A3 to help convey the intuition behind this operation.
    K. Qualitative results
We provide additional results from for all learned meth- ods, the color version of LGR, and Pixtransform – the strongest unsupervised competitor – for all three datasets and all four scaling factors in Table A5, Table A6, and Ta- ble A7.
The plots how that for smaller upsampling factors, all learned methods perform relatively well, while for larger






















































12	25	10


10


20	8

8
15	6

6
10	4
4
5	2
2









17.5

15.0

12.5

4	8	16	32
Upsampling factor
        (a) Middlebury

4	8	16	32
Upsampling factor
        (b) NYUv2
30


25


20









20.0

17.5

15.0

4	8	16	32
Upsampling factor
        (c) DIML

10.0

7.5

5.0

2.5


15


10


5
12.5

10.0

7.5

5.0

2.5

4	8	16	32
Upsampling factor
        (d) Middlebury

4	8	16	32
Upsampling factor
        (e) NYUv2

4	8	16	32
Upsampling factor
        (f) DIML

Figure A4. Curves with numbers from Tables 1 and 2 in main paper for better interpretability of results. RMSE is used instead of MSE for improved visualization across scales.














































Guide	Source	PixT	LGR†	DADA†	MSG	FDKN	PMBA	FDSR	LGR	DADA	GT
Table A5. Predictions and error maps for different guided super-resolution methods on the Middlebury dataset. Blue denotes under-estimated depth, red denotes over-estimation. All plots in a row have the same color scale. The last two columns juxtapose our predictions and the ground truth.














































Guide	Source	PixT	LGR†	DADA†	MSG	FDKN	PMBA	FDSR	LGR	DADA	GT














































Guide	Source	PixT	LGR†	DADA†	MSG	FDKN	PMBA	FDSR	LGR	DADA	GT